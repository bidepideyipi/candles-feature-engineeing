{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "model_training_header",
   "metadata": {},
   "source": [
    "# 03 - 模型训练模块\n",
    "\n",
    "此 notebook 负责使用生成的特征数据训练 XGBoost 分类模型，\n",
    "并对模型性能进行评估和优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9347b168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: matplotlib in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: joblib in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (1.5.3)\n",
      "Requirement already satisfied: xgboost in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (3.1.3)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.8.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: scipy in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from xgboost) (1.17.0)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.8.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: scipy in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from xgboost) (1.17.0)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anthony/Documents/github/technial_analysis_helper/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached scikit_learn-1.8.0-cp313-cp313-macosx_10_13_x86_64.whl (8.5 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached scikit_learn-1.8.0-cp313-cp313-macosx_10_13_x86_64.whl (8.5 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "\u001b[?25lInstalling collected packages: threadpoolctl, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed scikit-learn-1.8.0 threadpoolctl-3.6.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed scikit-learn-1.8.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn joblib xgboost scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "setup_environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "项目根目录: /Users/anthony/Documents/github/technial_analysis_helper\n",
      "模型训练环境设置完成\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 配置绘图样式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# 正确的路径设置\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"\\n项目根目录: {project_root}\")\n",
    "\n",
    "print(\"模型训练环境设置完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load_training_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载训练数据...\n",
      "✓ 从 CSV 文件成功加载训练数据\n",
      "  特征矩阵形状: (5275, 155)\n",
      "  目标向量长度: 5276\n",
      "\n",
      "数据概览:\n",
      "  样本数: 5,275\n",
      "  特征数: 155\n",
      "  缺失值: 0\n"
     ]
    }
   ],
   "source": [
    "# 加载训练数据\n",
    "print(\"正在加载训练数据...\")\n",
    "\n",
    "try:\n",
    "    # 尝试从 CSV 文件加载（如果之前已生成）\n",
    "    features_df = pd.read_csv('training_features.csv')\n",
    "    targets_series = pd.read_csv('training_targets.csv', header=None, names=['target'])['target']\n",
    "    \n",
    "    print(f\"✓ 从 CSV 文件成功加载训练数据\")\n",
    "    print(f\"  特征矩阵形状: {features_df.shape}\")\n",
    "    print(f\"  目标向量长度: {len(targets_series)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"CSV 文件不存在，尝试从数据库重新生成数据...\")\n",
    "    \n",
    "    # 从数据库加载并生成数据\n",
    "    from src.data.mongodb_handler import mongo_handler\n",
    "    from src.utils.feature_engineering import feature_engineer\n",
    "    from src.config.settings import config\n",
    "    \n",
    "    try:\n",
    "        if mongo_handler.connect():\n",
    "            # 加载足够多的数据\n",
    "            required_records = config.FEATURE_WINDOW_SIZE + 1000  # 额外缓冲\n",
    "            db_data = mongo_handler.get_candlestick_data(limit=required_records)\n",
    "            \n",
    "            if db_data:\n",
    "                print(f\"✓ 从数据库加载 {len(db_data)} 条记录\")\n",
    "                \n",
    "                # 转换数据格式\n",
    "                df_raw = pd.DataFrame(db_data)\n",
    "                df_raw = df_raw.sort_values('timestamp').reset_index(drop=True)\n",
    "                data_list = df_raw.to_dict('records')\n",
    "                \n",
    "                # 生成训练数据\n",
    "                features_df, targets_series = feature_engineer.create_training_dataset(\n",
    "                    data_list,\n",
    "                    stride=10,\n",
    "                    prediction_horizon=24\n",
    "                )\n",
    "                \n",
    "                if not features_df.empty:\n",
    "                    print(f\"✓ 成功生成训练数据: {features_df.shape}\")\n",
    "                    \n",
    "                    # 保存为 CSV 供后续使用\n",
    "                    features_df.to_csv('training_features.csv', index=False)\n",
    "                    targets_series.to_csv('training_targets.csv', index=False, header=True)\n",
    "                    print(\"✓ 数据已保存为 CSV 文件\")\n",
    "                else:\n",
    "                    print(\"✗ 无法生成训练数据\")\n",
    "            else:\n",
    "                print(\"✗ 数据库中没有足够数据\")\n",
    "                \n",
    "        mongo_handler.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ 加载数据时出错: {e}\")\n",
    "\n",
    "# 检查数据状态\n",
    "if 'features_df' in locals() and not features_df.empty:\n",
    "    print(f\"\\n数据概览:\")\n",
    "    print(f\"  样本数: {len(features_df):,}\")\n",
    "    print(f\"  特征数: {len(features_df.columns)}\")\n",
    "    print(f\"  缺失值: {features_df.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "data_exploration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在进行数据探索性分析...\n",
      "\n",
      "=== 数据基本统计 ===\n",
      "数据形状: (5275, 155)\n",
      "数值型特征数: 155\n",
      "非数值型特征数: 0\n",
      "\n",
      "=== 目标变量分布 ===\n",
      "类别 -1: 1118 样本 ( 21.2%)\n",
      "类别 -2:  805 样本 ( 15.3%)\n",
      "类别 -3:  390 样本 (  7.4%)\n",
      "类别 0:  882 样本 ( 16.7%)\n",
      "类别 1: 1051 样本 ( 19.9%)\n",
      "类别 2:  640 样本 ( 12.1%)\n",
      "类别 3:  389 样本 (  7.4%)\n",
      "类别 target:    1 样本 (  0.0%)\n",
      "\n",
      "=== 缺失值最多的前10个特征 ===\n",
      "                      缺失数量  缺失比例(%)\n",
      "short_rsi                0      0.0\n",
      "short_bb_upper           0      0.0\n",
      "short_bb_lower           0      0.0\n",
      "short_bb_middle          0      0.0\n",
      "short_bb_position        0      0.0\n",
      "short_price_ma_ratio     0      0.0\n",
      "medium_rsi               0      0.0\n",
      "medium_bb_upper          0      0.0\n",
      "medium_bb_lower          0      0.0\n",
      "medium_bb_middle         0      0.0\n",
      "\n",
      "=== 特征类型分析 ===\n",
      "技术指标(RSI): 3 个特征\n",
      "布林带指标: 12 个特征\n",
      "价格特征: 42 个特征\n",
      "原始价格序列: 131 个特征\n",
      "其他: 1 个特征\n",
      "\n",
      "=== 数值特征描述性统计 ===\n",
      "       short_rsi  short_bb_upper  short_bb_lower  short_bb_middle  \\\n",
      "count  5275.0000       5275.0000       5275.0000        5275.0000   \n",
      "mean     51.0347       2221.4350       2119.6219        2170.5284   \n",
      "std      14.6475       1200.8698       1147.7316        1173.5175   \n",
      "min       2.9992        118.4397         96.6586         111.9050   \n",
      "25%      41.1992       1551.8209       1429.6549        1497.3890   \n",
      "50%      50.8961       2131.8494       2038.6513        2083.8330   \n",
      "75%      60.9625       3155.6728       3015.9043        3093.6180   \n",
      "max      94.3021       5036.5842       4752.2784        4792.8565   \n",
      "\n",
      "       short_bb_position  short_price_ma_ratio  medium_rsi  medium_bb_upper  \\\n",
      "count          5275.0000             5275.0000   5275.0000        5275.0000   \n",
      "mean              0.5150                1.0007     50.9709        2221.4350   \n",
      "std               0.3373                0.0211     12.3819        1200.8698   \n",
      "min              -0.5035                0.8348      7.4657         118.4397   \n",
      "25%               0.2586                0.9919     42.8868        1551.8209   \n",
      "50%               0.5333                1.0007     51.0479        2131.8494   \n",
      "75%               0.7667                1.0101     59.0574        3155.6728   \n",
      "max               1.5469                1.1506     91.2786        5036.5842   \n",
      "\n",
      "       medium_bb_lower  medium_bb_middle  ...  raw_price_min  raw_price_max  \\\n",
      "count        5275.0000         5275.0000  ...      5275.0000      5275.0000   \n",
      "mean         2119.6219         2170.5284  ...      1945.2928      2375.0607   \n",
      "std          1147.7316         1173.5175  ...      1062.7966      1280.7915   \n",
      "min            96.6586          111.9050  ...        95.5200       140.7800   \n",
      "25%          1429.6549         1497.3890  ...      1269.7300      1660.7700   \n",
      "50%          2038.6513         2083.8330  ...      1842.6400      2377.1600   \n",
      "75%          3015.9043         3093.6180  ...      2780.1000      3395.8000   \n",
      "max          4752.2784         4792.8565  ...      4362.9500      4934.3000   \n",
      "\n",
      "       raw_price_change_mean  raw_price_change_std  raw_price_change_sum  \\\n",
      "count              5275.0000             5275.0000             5275.0000   \n",
      "mean                  0.0555               16.3806               16.5916   \n",
      "std                   1.1302               11.0151              337.9374   \n",
      "min                  -6.6831                0.6598            -1998.2500   \n",
      "25%                  -0.4523                7.6834             -135.2450   \n",
      "50%                   0.0474               15.5059               14.1800   \n",
      "75%                   0.5640               23.6939              168.6350   \n",
      "max                   5.3813               76.1258             1609.0000   \n",
      "\n",
      "       raw_volume_mean  raw_volume_std  raw_price_range_mean  \\\n",
      "count     5.275000e+03    5.275000e+03             5275.0000   \n",
      "mean      1.357251e+06    1.114189e+06               24.4220   \n",
      "std       2.813193e+06    2.088541e+06               16.6725   \n",
      "min       8.876243e+04    6.448574e+04                0.9507   \n",
      "25%       4.014421e+05    3.417218e+05               10.4529   \n",
      "50%       8.159986e+05    7.661186e+05               22.9539   \n",
      "75%       1.208077e+06    1.190119e+06               35.7277   \n",
      "max       2.930276e+07    2.492814e+07              119.9392   \n",
      "\n",
      "       raw_price_range_max     timestamp  \n",
      "count            5275.0000  5.275000e+03  \n",
      "mean              142.2468  1.673240e+12  \n",
      "std               119.5860  5.482460e+10  \n",
      "min                 4.1300  1.578308e+12  \n",
      "25%                69.2750  1.625774e+12  \n",
      "50%               117.0000  1.673240e+12  \n",
      "75%               180.2200  1.720706e+12  \n",
      "max               811.7200  1.768172e+12  \n",
      "\n",
      "[8 rows x 155 columns]\n",
      "       short_rsi  short_bb_upper  short_bb_lower  short_bb_middle  \\\n",
      "count  5275.0000       5275.0000       5275.0000        5275.0000   \n",
      "mean     51.0347       2221.4350       2119.6219        2170.5284   \n",
      "std      14.6475       1200.8698       1147.7316        1173.5175   \n",
      "min       2.9992        118.4397         96.6586         111.9050   \n",
      "25%      41.1992       1551.8209       1429.6549        1497.3890   \n",
      "50%      50.8961       2131.8494       2038.6513        2083.8330   \n",
      "75%      60.9625       3155.6728       3015.9043        3093.6180   \n",
      "max      94.3021       5036.5842       4752.2784        4792.8565   \n",
      "\n",
      "       short_bb_position  short_price_ma_ratio  medium_rsi  medium_bb_upper  \\\n",
      "count          5275.0000             5275.0000   5275.0000        5275.0000   \n",
      "mean              0.5150                1.0007     50.9709        2221.4350   \n",
      "std               0.3373                0.0211     12.3819        1200.8698   \n",
      "min              -0.5035                0.8348      7.4657         118.4397   \n",
      "25%               0.2586                0.9919     42.8868        1551.8209   \n",
      "50%               0.5333                1.0007     51.0479        2131.8494   \n",
      "75%               0.7667                1.0101     59.0574        3155.6728   \n",
      "max               1.5469                1.1506     91.2786        5036.5842   \n",
      "\n",
      "       medium_bb_lower  medium_bb_middle  ...  raw_price_min  raw_price_max  \\\n",
      "count        5275.0000         5275.0000  ...      5275.0000      5275.0000   \n",
      "mean         2119.6219         2170.5284  ...      1945.2928      2375.0607   \n",
      "std          1147.7316         1173.5175  ...      1062.7966      1280.7915   \n",
      "min            96.6586          111.9050  ...        95.5200       140.7800   \n",
      "25%          1429.6549         1497.3890  ...      1269.7300      1660.7700   \n",
      "50%          2038.6513         2083.8330  ...      1842.6400      2377.1600   \n",
      "75%          3015.9043         3093.6180  ...      2780.1000      3395.8000   \n",
      "max          4752.2784         4792.8565  ...      4362.9500      4934.3000   \n",
      "\n",
      "       raw_price_change_mean  raw_price_change_std  raw_price_change_sum  \\\n",
      "count              5275.0000             5275.0000             5275.0000   \n",
      "mean                  0.0555               16.3806               16.5916   \n",
      "std                   1.1302               11.0151              337.9374   \n",
      "min                  -6.6831                0.6598            -1998.2500   \n",
      "25%                  -0.4523                7.6834             -135.2450   \n",
      "50%                   0.0474               15.5059               14.1800   \n",
      "75%                   0.5640               23.6939              168.6350   \n",
      "max                   5.3813               76.1258             1609.0000   \n",
      "\n",
      "       raw_volume_mean  raw_volume_std  raw_price_range_mean  \\\n",
      "count     5.275000e+03    5.275000e+03             5275.0000   \n",
      "mean      1.357251e+06    1.114189e+06               24.4220   \n",
      "std       2.813193e+06    2.088541e+06               16.6725   \n",
      "min       8.876243e+04    6.448574e+04                0.9507   \n",
      "25%       4.014421e+05    3.417218e+05               10.4529   \n",
      "50%       8.159986e+05    7.661186e+05               22.9539   \n",
      "75%       1.208077e+06    1.190119e+06               35.7277   \n",
      "max       2.930276e+07    2.492814e+07              119.9392   \n",
      "\n",
      "       raw_price_range_max     timestamp  \n",
      "count            5275.0000  5.275000e+03  \n",
      "mean              142.2468  1.673240e+12  \n",
      "std               119.5860  5.482460e+10  \n",
      "min                 4.1300  1.578308e+12  \n",
      "25%                69.2750  1.625774e+12  \n",
      "50%               117.0000  1.673240e+12  \n",
      "75%               180.2200  1.720706e+12  \n",
      "max               811.7200  1.768172e+12  \n",
      "\n",
      "[8 rows x 155 columns]\n"
     ]
    }
   ],
   "source": [
    "# 数据探索性分析\n",
    "print(\"正在进行数据探索性分析...\")\n",
    "\n",
    "if 'features_df' in locals() and not features_df.empty:\n",
    "    # 基本统计信息\n",
    "    print(\"\\n=== 数据基本统计 ===\")\n",
    "    print(f\"数据形状: {features_df.shape}\")\n",
    "    print(f\"数值型特征数: {len(features_df.select_dtypes(include=[np.number]).columns)}\")\n",
    "    print(f\"非数值型特征数: {len(features_df.select_dtypes(exclude=[np.number]).columns)}\")\n",
    "    \n",
    "    # 目标变量分布\n",
    "    print(f\"\\n=== 目标变量分布 ===\")\n",
    "    target_dist = targets_series.value_counts().sort_index()\n",
    "    for label, count in target_dist.items():\n",
    "        percentage = (count / len(targets_series)) * 100\n",
    "        print(f\"类别 {label}: {count:4d} 样本 ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # 缺失值分析\n",
    "    missing_data = features_df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(features_df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        '缺失数量': missing_data,\n",
    "        '缺失比例(%)': missing_percent\n",
    "    }).sort_values('缺失比例(%)', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== 缺失值最多的前10个特征 ===\")\n",
    "    print(missing_summary.head(10))\n",
    "    \n",
    "    # 特征类型分析\n",
    "    print(f\"\\n=== 特征类型分析 ===\")\n",
    "    feature_categories = {\n",
    "        '技术指标(RSI)': [col for col in features_df.columns if 'rsi' in col.lower()],\n",
    "        '布林带指标': [col for col in features_df.columns if 'bb_' in col.lower()],\n",
    "        '价格特征': [col for col in features_df.columns if any(prefix in col for prefix in \n",
    "                     ['current_', 'price_', 'volume_', 'volatility'])],\n",
    "        '原始价格序列': [col for col in features_df.columns if col.startswith('raw_')],\n",
    "        '其他': [col for col in features_df.columns if col == 'timestamp']\n",
    "    }\n",
    "    \n",
    "    for category, cols in feature_categories.items():\n",
    "        if cols:\n",
    "            print(f\"{category}: {len(cols)} 个特征\")\n",
    "    \n",
    "    # 数值特征的基本统计\n",
    "    numeric_features = features_df.select_dtypes(include=[np.number])\n",
    "    print(f\"\\n=== 数值特征描述性统计 ===\")\n",
    "    print(numeric_features.describe().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "data_preprocessing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在进行数据预处理...\n",
      "处理缺失值...\n",
      "  填充前缺失值: 0\n",
      "  处理后缺失值: 0\n",
      "  已移除 timestamp 列\n",
      "\n",
      "预处理后的数据形状: (5275, 154)\n",
      "✓ 预处理后的数据已保存\n",
      "✓ 预处理后的数据已保存\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "print(\"正在进行数据预处理...\")\n",
    "\n",
    "if 'features_df' in locals() and not features_df.empty:\n",
    "    # 处理缺失值\n",
    "    print(\"处理缺失值...\")\n",
    "    initial_missing = features_df.isnull().sum().sum()\n",
    "    \n",
    "    # 对于数值型特征，用中位数填充\n",
    "    numeric_columns = features_df.select_dtypes(include=[np.number]).columns\n",
    "    features_processed = features_df.copy()\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if features_processed[col].isnull().any():\n",
    "            median_value = features_processed[col].median()\n",
    "            features_processed[col].fillna(median_value, inplace=True)\n",
    "    \n",
    "    # 移除仍含有缺失值的列（通常是字符串或其他类型）\n",
    "    features_processed = features_processed.dropna(axis=1)\n",
    "    \n",
    "    final_missing = features_processed.isnull().sum().sum()\n",
    "    print(f\"  填充前缺失值: {initial_missing:,}\")\n",
    "    print(f\"  处理后缺失值: {final_missing:,}\")\n",
    "    \n",
    "    # 移除时间戳列（通常不用于训练）\n",
    "    if 'timestamp' in features_processed.columns:\n",
    "        features_processed = features_processed.drop('timestamp', axis=1)\n",
    "        print(\"  已移除 timestamp 列\")\n",
    "    \n",
    "    print(f\"\\n预处理后的数据形状: {features_processed.shape}\")\n",
    "    \n",
    "    # 检查是否有常数特征（方差为0）\n",
    "    constant_features = []\n",
    "    for col in features_processed.select_dtypes(include=[np.number]).columns:\n",
    "        if features_processed[col].var() == 0:\n",
    "            constant_features.append(col)\n",
    "    \n",
    "    if constant_features:\n",
    "        print(f\"\\n发现 {len(constant_features)} 个常数特征，正在移除...\")\n",
    "        features_processed = features_processed.drop(columns=constant_features)\n",
    "        print(f\"移除后数据形状: {features_processed.shape}\")\n",
    "    \n",
    "    # 保存预处理后的数据\n",
    "    features_processed.to_csv('processed_features.csv', index=False)\n",
    "    targets_series.to_csv('processed_targets.csv', index=False, header=True)\n",
    "    print(\"✓ 预处理后的数据已保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "model_training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始模型训练...\n",
      "✗ 模型训练时出错: Found input variables with inconsistent numbers of samples: [5275, 5276]\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "from src.models.xgboost_trainer import xgb_trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"开始模型训练...\")\n",
    "\n",
    "if 'features_processed' in locals() and not features_processed.empty:\n",
    "    try:\n",
    "        # 分割训练集和测试集\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features_processed, targets_series, \n",
    "            test_size=0.2, \n",
    "            random_state=42, \n",
    "            stratify=targets_series\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n数据分割完成:\")\n",
    "        print(f\"  训练集: {X_train.shape[0]:,} 样本\")\n",
    "        print(f\"  测试集: {X_test.shape[0]:,} 样本\")\n",
    "        print(f\"  特征数: {X_train.shape[1]}\")\n",
    "        \n",
    "        # 训练模型\n",
    "        print(f\"\\n正在训练 XGBoost 模型...\")\n",
    "        results = xgb_trainer.train_model(X_train, y_train)\n",
    "        \n",
    "        print(f\"\\n=== 训练结果 ===\")\n",
    "        print(f\"训练准确率: {results['accuracy']:.4f}\")\n",
    "        print(f\"交叉验证准确率: {results['cv_mean_accuracy']:.4f} ± {results['cv_std_accuracy']:.4f}\")\n",
    "        \n",
    "        # 显示各类别置信度\n",
    "        print(f\"\\n各类别置信度:\")\n",
    "        for class_label, confidence in results['class_confidence'].items():\n",
    "            print(f\"  类别 {class_label}: {confidence:.4f}\")\n",
    "            \n",
    "        # 在测试集上评估\n",
    "        print(f\"\\n在测试集上评估模型...\")\n",
    "        test_predictions, test_probabilities = xgb_trainer.predict(X_test)\n",
    "        \n",
    "        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "        \n",
    "        test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "        print(f\"测试集准确率: {test_accuracy:.4f}\")\n",
    "        \n",
    "        print(f\"\\n分类报告:\")\n",
    "        print(classification_report(y_test, test_predictions, digits=4))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ 模型训练时出错: {e}\")\n",
    "else:\n",
    "    print(\"没有可用的预处理数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型详细评估\n",
    "print(\"正在进行详细的模型评估...\")\n",
    "\n",
    "if 'test_predictions' in locals():\n",
    "    try:\n",
    "        # 混淆矩阵\n",
    "        cm = confusion_matrix(y_test, test_predictions)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=sorted(y_test.unique()), \n",
    "                   yticklabels=sorted(y_test.unique()))\n",
    "        plt.title('混淆矩阵')\n",
    "        plt.ylabel('真实标签')\n",
    "        plt.xlabel('预测标签')\n",
    "        plt.show()\n",
    "        \n",
    "        # 各类别预测概率分布\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # 计算每个真实类别的平均预测概率\n",
    "        unique_classes = sorted(y_test.unique())\n",
    "        avg_probs_by_class = {}\n",
    "        \n",
    "        for true_class in unique_classes:\n",
    "            mask = (y_test == true_class)\n",
    "            if mask.sum() > 0:\n",
    "                # 获取该类别样本的所有预测概率\n",
    "                class_probs = test_probabilities[mask]\n",
    "                # 计算每列的平均值（每个预测类别的平均概率）\n",
    "                avg_probs_by_class[true_class] = class_probs.mean(axis=0)\n",
    "        \n",
    "        # 绘制热力图\n",
    "        if avg_probs_by_class:\n",
    "            prob_matrix = np.array([avg_probs_by_class[c] for c in unique_classes])\n",
    "            \n",
    "            sns.heatmap(prob_matrix, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "                       xticklabels=unique_classes,\n",
    "                       yticklabels=unique_classes)\n",
    "            plt.title('各类别平均预测概率矩阵')\n",
    "            plt.xlabel('预测类别')\n",
    "            plt.ylabel('真实类别')\n",
    "            plt.show()\n",
    "        \n",
    "        # 特征重要性\n",
    "        if hasattr(xgb_trainer.model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance': xgb_trainer.model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            top_features = feature_importance.head(20)\n",
    "            sns.barplot(data=top_features, y='feature', x='importance')\n",
    "            plt.title('Top 20 特征重要性')\n",
    "            plt.xlabel('重要性')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nTop 10 重要特征:\")\n",
    "            for idx, row in feature_importance.head(10).iterrows():\n",
    "                print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ 模型评估时出错: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross_validation_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉验证详细分析\n",
    "print(\"正在进行交叉验证分析...\")\n",
    "\n",
    "if 'results' in locals() and 'cv_scores' in results:\n",
    "    try:\n",
    "        cv_scores = results['cv_scores']\n",
    "        \n",
    "        print(f\"\\n=== 交叉验证结果 ===\")\n",
    "        print(f\"各折准确率: {[f'{score:.4f}' for score in cv_scores]}\")\n",
    "        print(f\"平均准确率: {np.mean(cv_scores):.4f}\")\n",
    "        print(f\"标准差: {np.std(cv_scores):.4f}\")\n",
    "        print(f\"最小值: {np.min(cv_scores):.4f}\")\n",
    "        print(f\"最大值: {np.max(cv_scores):.4f}\")\n",
    "        \n",
    "        # 绘制交叉验证分数分布\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.boxplot(cv_scores, vert=False)\n",
    "        plt.scatter(cv_scores, [1]*len(cv_scores), alpha=0.6, s=50)\n",
    "        plt.xlabel('准确率')\n",
    "        plt.title('交叉验证准确率分布')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # 各类别交叉验证表现\n",
    "        if 'cv_class_reports' in results:\n",
    "            print(f\"\\n各类别交叉验证平均表现:\")\n",
    "            class_metrics = {}\n",
    "            \n",
    "            for report in results['cv_class_reports']:\n",
    "                for class_label, metrics in report.items():\n",
    "                    if class_label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                        if class_label not in class_metrics:\n",
    "                            class_metrics[class_label] = {'precision': [], 'recall': [], 'f1-score': []}\n",
    "                        \n",
    "                        class_metrics[class_label]['precision'].append(metrics['precision'])\n",
    "                        class_metrics[class_label]['recall'].append(metrics['recall'])\n",
    "                        class_metrics[class_label]['f1-score'].append(metrics['f1-score'])\n",
    "            \n",
    "            # 计算平均值和标准差\n",
    "            for class_label, metrics in class_metrics.items():\n",
    "                print(f\"\\n类别 {class_label}:\")\n",
    "                for metric_name, values in metrics.items():\n",
    "                    mean_val = np.mean(values)\n",
    "                    std_val = np.std(values)\n",
    "                    print(f\"  {metric_name}: {mean_val:.4f} ± {std_val:.4f}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ 交叉验证分析时出错: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_saving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "print(\"正在保存训练好的模型...\")\n",
    "\n",
    "try:\n",
    "    # 保存模型\n",
    "    save_success = xgb_trainer.save_model()\n",
    "    \n",
    "    if save_success:\n",
    "        print(\"✓ 模型保存成功\")\n",
    "        \n",
    "        # 验证模型加载\n",
    "        print(\"验证模型加载...\")\n",
    "        load_success = xgb_trainer.load_model()\n",
    "        \n",
    "        if load_success:\n",
    "            print(\"✓ 模型加载验证成功\")\n",
    "        else:\n",
    "            print(\"✗ 模型加载验证失败\")\n",
    "    else:\n",
    "        print(\"✗ 模型保存失败\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ 模型保存时出错: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_model_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最终模型总结\n",
    "print(\"=== 模型训练总结 ===\")\n",
    "\n",
    "if 'results' in locals():\n",
    "    print(f\"\\n📊 模型性能指标:\")\n",
    "    print(f\"  训练集准确率: {results['accuracy']:.4f}\")\n",
    "    print(f\"  交叉验证准确率: {results['cv_mean_accuracy']:.4f} ± {results['cv_std_accuracy']:.4f}\")\n",
    "    \n",
    "    if 'test_accuracy' in locals():\n",
    "        print(f\"  测试集准确率: {test_accuracy:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📈 训练数据信息:\")\n",
    "    print(f\"  训练样本数: {len(X_train):,}\")\n",
    "    print(f\"  测试样本数: {len(X_test):,}\")\n",
    "    print(f\"  特征数量: {X_train.shape[1]}\")\n",
    "    \n",
    "    print(f\"\\n🎯 目标类别分布:\")\n",
    "    for label, count in targets_series.value_counts().sort_index().items():\n",
    "        percentage = (count / len(targets_series)) * 100\n",
    "        print(f\"  类别 {label}: {count:,} 样本 ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n💾 模型文件:\")\n",
    "    from src.config.settings import config\n",
    "    model_path = Path(config.MODEL_SAVE_PATH)\n",
    "    if model_path.exists():\n",
    "        size_mb = model_path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  模型文件: {model_path}\")\n",
    "        print(f\"  文件大小: {size_mb:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\n✅ 训练完成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # 生成性能建议\n",
    "    print(f\"\\n💡 性能建议:\")\n",
    "    \n",
    "    accuracy = results['accuracy']\n",
    "    cv_std = results['cv_std_accuracy']\n",
    "    \n",
    "    if accuracy >= 0.7:\n",
    "        print(\"  ✓ 模型性能良好，可以用于实际预测\")\n",
    "    elif accuracy >= 0.6:\n",
    "        print(\"  ⚠️ 模型性能一般，建议收集更多数据或调整特征\")\n",
    "    else:\n",
    "        print(\"  ✗ 模型性能较差，建议重新设计特征或调整模型参数\")\n",
    "    \n",
    "    if cv_std <= 0.05:\n",
    "        print(\"  ✓ 模型稳定性良好\")\n",
    "    else:\n",
    "        print(\"  ⚠️ 模型可能存在过拟合，建议增加正则化或减少特征\")\n",
    "\n",
    "print(f\"\\n=== 下一步建议 ===\")\n",
    "print(\"1. 运行 `04_prediction.ipynb` 进行实时预测\")\n",
    "print(\"2. 如需改进模型，可以返回特征工程阶段调整特征\")\n",
    "print(\"3. 定期重新训练模型以适应市场变化\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
